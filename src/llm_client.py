#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM å®¢æˆ·ç«¯å°è£…
æ”¯æŒå¤šç§å¤§æ¨¡å‹ APIï¼šOpenAIã€Claudeã€Qwenã€DeepSeekã€æœ¬åœ°æ¨¡å‹ç­‰
"""

import os
from typing import Optional, Callable
from pathlib import Path


class LLMClient:
    """
    ç»Ÿä¸€çš„ LLM å®¢æˆ·ç«¯æ¥å£
    æ”¯æŒå¤šç§å¤§æ¨¡å‹ API
    """
    
    def __init__(
        self,
        provider: str = "openai",
        model: str = None,
        api_key: str = None,
        base_url: str = None,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ):
        """
        åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        
        Args:
            provider: æä¾›å•† ("openai", "qwen", "deepseek", "claude", "local")
            model: æ¨¡å‹åç§°
            api_key: API å¯†é’¥ï¼ˆä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
            base_url: API åŸºç¡€ URL
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
        """
        self.provider = provider.lower()
        self.temperature = temperature
        self.max_tokens = max_tokens
        
        # é»˜è®¤é…ç½®
        self.configs = {
            "openai": {
                "model": "gpt-4o-mini",
                "base_url": "https://api.openai.com/v1",
                "env_key": "OPENAI_API_KEY"
            },
            "qwen": {
                "model": "qwen-turbo",
                "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
                "env_key": "DASHSCOPE_API_KEY"
            },
            "deepseek": {
                "model": "deepseek-chat",
                "base_url": "https://api.deepseek.com/v1",
                "env_key": "DEEPSEEK_API_KEY"
            },
            "claude": {
                "model": "claude-3-haiku-20240307",
                "base_url": None,  # ä½¿ç”¨å®˜æ–¹ SDK
                "env_key": "ANTHROPIC_API_KEY"
            },
            # ========== å…è´¹ API ==========
            # ç¡…åŸºæµåŠ¨ - æ³¨å†Œé€é¢åº¦ï¼Œæ”¯æŒå¤šç§å¼€æºæ¨¡å‹
            # ç”³è¯·: https://cloud.siliconflow.cn/
            "siliconflow": {
                "model": "Qwen/Qwen2.5-7B-Instruct",
                "base_url": "https://api.siliconflow.cn/v1",
                "env_key": "SILICONFLOW_API_KEY"
            },
            # Groq - å…è´¹ä½¿ç”¨ Llama/Mixtral
            # ç”³è¯·: https://console.groq.com/
            "groq": {
                "model": "llama-3.1-8b-instant",
                "base_url": "https://api.groq.com/openai/v1",
                "env_key": "GROQ_API_KEY"
            },
            # æ™ºè°± GLM - æ–°ç”¨æˆ·æœ‰å…è´¹é¢åº¦
            # ç”³è¯·: https://open.bigmodel.cn/
            "zhipu": {
                "model": "glm-4-flash",
                "base_url": "https://open.bigmodel.cn/api/paas/v4",
                "env_key": "ZHIPU_API_KEY"
            },
            # Google Gemini - å…è´¹é¢åº¦å……è¶³
            # ç”³è¯·: https://aistudio.google.com/apikey
            "gemini": {
                "model": "gemini-2.0-flash",
                "base_url": "https://generativelanguage.googleapis.com/v1beta/openai",
                "env_key": "GEMINI_API_KEY"
            },
            # ========== æœ¬åœ°æ¨¡å‹ ==========
            "local": {
                "model": "default",
                "base_url": "http://localhost:8080/v1",
                "env_key": None
            },
            "ollama": {
                "model": "qwen2.5:7b",
                "base_url": "http://localhost:11434/v1",
                "env_key": None
            }
        }
        
        # è·å–é…ç½®
        config = self.configs.get(self.provider, self.configs["openai"])
        
        self.model = model or config["model"]
        self.base_url = base_url or config["base_url"]
        self.api_key = api_key or os.getenv(config["env_key"] or "", "sk-placeholder")
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self.client = None
        self._init_client()
        
        print(f"âœ… LLM å®¢æˆ·ç«¯åˆå§‹åŒ–: {self.provider} / {self.model}")
    
    def _init_client(self):
        """åˆå§‹åŒ–å…·ä½“çš„å®¢æˆ·ç«¯"""
        if self.provider == "claude":
            try:
                import anthropic
                self.client = anthropic.Anthropic(api_key=self.api_key)
            except ImportError:
                print("âš ï¸ è¯·å®‰è£… anthropic: pip install anthropic")
        else:
            # OpenAI å…¼å®¹æ¥å£
            try:
                import openai
                self.client = openai.OpenAI(
                    api_key=self.api_key,
                    base_url=self.base_url
                )
            except ImportError:
                print("âš ï¸ è¯·å®‰è£… openai: pip install openai")
    
    def chat(self, prompt: str, system: str = None) -> str:
        """
        å‘é€å¯¹è¯è¯·æ±‚
        
        Args:
            prompt: ç”¨æˆ·æç¤º
            system: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
        
        Returns:
            æ¨¡å‹å“åº”æ–‡æœ¬
        """
        if self.client is None:
            raise RuntimeError("LLM å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        
        try:
            if self.provider == "claude":
                # Claude API
                messages = [{"role": "user", "content": prompt}]
                response = self.client.messages.create(
                    model=self.model,
                    max_tokens=self.max_tokens,
                    system=system or "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦åŠ©æ‰‹ã€‚",
                    messages=messages
                )
                return response.content[0].text
            else:
                # OpenAI å…¼å®¹æ¥å£
                messages = []
                if system:
                    messages.append({"role": "system", "content": system})
                messages.append({"role": "user", "content": prompt})
                
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=self.temperature,
                    max_tokens=self.max_tokens
                )
                return response.choices[0].message.content
        
        except Exception as e:
            print(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def __call__(self, prompt: str) -> str:
        """å…è®¸ç›´æ¥è°ƒç”¨"""
        return self.chat(prompt)


# ============================================
# ä¾¿æ·å‡½æ•°
# ============================================

def create_llm_api(
    provider: str = "qwen",
    model: str = None,
    api_key: str = None
) -> Callable[[str], str]:
    """
    åˆ›å»º LLM API å‡½æ•°ï¼Œä¾› DiaAgent ä½¿ç”¨
    
    Args:
        provider: æä¾›å•†åç§°
        model: æ¨¡å‹åç§°
        api_key: API å¯†é’¥
    
    Returns:
        å¯è°ƒç”¨çš„ LLM å‡½æ•°
    
    Usage:
        llm_api = create_llm_api("qwen")
        agent = DiaAgent(llm_api=llm_api)
    """
    client = LLMClient(provider=provider, model=model, api_key=api_key)
    return client


def create_qwen_api(api_key: str = None, model: str = "qwen-turbo") -> Callable[[str], str]:
    """åˆ›å»ºé€šä¹‰åƒé—® API"""
    return create_llm_api("qwen", model=model, api_key=api_key)


def create_deepseek_api(api_key: str = None, model: str = "deepseek-chat") -> Callable[[str], str]:
    """åˆ›å»º DeepSeek API"""
    return create_llm_api("deepseek", model=model, api_key=api_key)


def create_openai_api(api_key: str = None, model: str = "gpt-4o-mini") -> Callable[[str], str]:
    """åˆ›å»º OpenAI API"""
    return create_llm_api("openai", model=model, api_key=api_key)


def create_ollama_api(model: str = "qwen2.5:7b", base_url: str = None) -> Callable[[str], str]:
    """åˆ›å»º Ollama æœ¬åœ° API"""
    client = LLMClient(
        provider="ollama",
        model=model,
        base_url=base_url or "http://localhost:11434/v1",
        api_key="ollama"
    )
    return client


# ========== å…è´¹ API ä¾¿æ·å‡½æ•° ==========

def create_siliconflow_api(api_key: str = None, model: str = "Qwen/Qwen2.5-7B-Instruct") -> Callable[[str], str]:
    """
    åˆ›å»ºç¡…åŸºæµåŠ¨ API (å…è´¹é¢åº¦)
    ç”³è¯·åœ°å€: https://cloud.siliconflow.cn/
    """
    return create_llm_api("siliconflow", model=model, api_key=api_key)


def create_groq_api(api_key: str = None, model: str = "llama-3.1-8b-instant") -> Callable[[str], str]:
    """
    åˆ›å»º Groq API (å…è´¹)
    ç”³è¯·åœ°å€: https://console.groq.com/
    """
    return create_llm_api("groq", model=model, api_key=api_key)


def create_zhipu_api(api_key: str = None, model: str = "glm-4-flash") -> Callable[[str], str]:
    """
    åˆ›å»ºæ™ºè°± GLM API (æ–°ç”¨æˆ·æœ‰å…è´¹é¢åº¦)
    ç”³è¯·åœ°å€: https://open.bigmodel.cn/
    """
    return create_llm_api("zhipu", model=model, api_key=api_key)


def create_gemini_api(api_key: str = None, model: str = "gemini-2.0-flash") -> Callable[[str], str]:
    """
    åˆ›å»º Google Gemini API (å…è´¹é¢åº¦å……è¶³)
    ç”³è¯·åœ°å€: https://aistudio.google.com/apikey
    """
    return create_llm_api("gemini", model=model, api_key=api_key)


# ============================================
# æµ‹è¯•
# ============================================

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ§ª LLM å®¢æˆ·ç«¯æµ‹è¯•")
    print("=" * 60)
    
    # æ£€æŸ¥å¯ç”¨çš„ API å¯†é’¥ (ä¼˜å…ˆæ£€æŸ¥å…è´¹ API)
    providers = [
        ("siliconflow", "SILICONFLOW_API_KEY", "ç¡…åŸºæµåŠ¨ (å…è´¹)"),
        ("groq", "GROQ_API_KEY", "Groq (å…è´¹)"),
        ("zhipu", "ZHIPU_API_KEY", "æ™ºè°± GLM"),
        ("qwen", "DASHSCOPE_API_KEY", "é€šä¹‰åƒé—®"),
        ("deepseek", "DEEPSEEK_API_KEY", "DeepSeek"),
        ("openai", "OPENAI_API_KEY", "OpenAI"),
    ]
    
    print("\nğŸ“‹ API é…ç½®çŠ¶æ€:")
    available = None
    for provider, env_key, name in providers:
        if os.getenv(env_key):
            print(f"  âœ… {name}: å·²é…ç½®")
            if available is None:
                available = provider
        else:
            print(f"  âŒ {name}: æœªé…ç½®")
    
    if available:
        print(f"\nğŸ“ æµ‹è¯• {available} API...")
        try:
            llm = create_llm_api(available)
            response = llm("è¯·ç”¨ä¸€å¥è¯ä»‹ç»ç³–å°¿ç—…")
            print(f"å“åº”: {response[:200]}...")
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    else:
        print("\n" + "=" * 60)
        print("âš ï¸ æœªæ£€æµ‹åˆ°å¯ç”¨çš„ API å¯†é’¥")
        print("=" * 60)
        print("\næ¨èå…è´¹ API (æ³¨å†Œå³ç”¨):")
        print("  1. ç¡…åŸºæµåŠ¨: https://cloud.siliconflow.cn/")
        print("     export SILICONFLOW_API_KEY=your-key")
        print("  2. Groq: https://console.groq.com/")
        print("     export GROQ_API_KEY=your-key")
        print("  3. æ™ºè°± GLM: https://open.bigmodel.cn/")
        print("     export ZHIPU_API_KEY=your-key")

